{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary -\n",
    "# - There are totally 59K touch gestures.\n",
    "# - Only ~22K (37%) touch gestures was clicked on a leaf element which had text content.\n",
    "# - Created text dataset in the following format\n",
    "#   - [[e11, e21, ... e1(MAX_TOKEN), e21, e22, ... e2(MAX_TOKEN), TARGET_TEXT],\n",
    "#      ...\n",
    "#     ]\n",
    "#   - Vectorized the dataset.\n",
    "# - Tried a simple classification model with a single hidden layer(1024).\n",
    "#   - Accuracy on train data : 48%\n",
    "#   - Accuracy on validation (20%) : 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements.\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants.\n",
    "LONG_TOUCH_THRESHOLD = 5\n",
    "DIM_X = 1440\n",
    "DIM_Y = 2560\n",
    "MAX_TOKEN = 64\n",
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 100\n",
    "VOCAB_SIZE = 500\n",
    "TRAIN_SIZE = 0.8\n",
    "VAL_SIZE = 0.1\n",
    "TEST_SIZE = 0.1\n",
    "TRACES_PATH = 'filtered_traces/*/*'\n",
    "NEGATIVE_SAMPLE_TARGET = '[null]'\n",
    "PLACEHOLDER_TEXT = 'n/a'\n",
    "y_true = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets all leaf nodes for a given element.\n",
    "def get_leaf_nodes(element, leaf_nodes):\n",
    "    if not element:\n",
    "        return leaf_nodes\n",
    "    if 'children' not in element:\n",
    "        leaf_nodes.append(element)\n",
    "        return leaf_nodes\n",
    "    for child in element['children']:\n",
    "        get_leaf_nodes(child, leaf_nodes)\n",
    "    return leaf_nodes\n",
    "\n",
    "\n",
    "def get_all_leaf_nodes(view_hierarchy_json):\n",
    "    activity = view_hierarchy_json.get('activity')\n",
    "    if not activity:\n",
    "        return dataset\n",
    "    root = activity.get('root')\n",
    "    return get_leaf_nodes(root, [])\n",
    "\n",
    "\n",
    "def get_target_text(leaf_nodes, x, y):\n",
    "    target_text = None\n",
    "    for leaf_node in leaf_nodes:\n",
    "        bounds = leaf_node['bounds']\n",
    "        if bounds[0] <= x and bounds[2] >= x and bounds[1] <= y and bounds[3] >= y:\n",
    "            if 'text' in leaf_node:\n",
    "                target_text = leaf_node['text'] or leaf_node.get('text-hint')\n",
    "    return target_text\n",
    "\n",
    "\n",
    "def get_leaf_node_texts(leaf_nodes):\n",
    "    i = 1\n",
    "    element_texts = []\n",
    "    for leaf_node in leaf_nodes:\n",
    "        if 'text' in leaf_node:\n",
    "            text = leaf_node['text'] or leaf_node.get('text-hint')\n",
    "            element_texts.append(str(text))\n",
    "            i += 1\n",
    "            if i == MAX_TOKEN:\n",
    "                break\n",
    "    return element_texts\n",
    "\n",
    "\n",
    "# Identifies if a given gesture is a TOUCH gesture.\n",
    "# In this task, we will only be focussing on TOUCH gestures.\n",
    "def is_touch_gesture(gesture):\n",
    "    if len(gesture) <= LONG_TOUCH_THRESHOLD:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of touch gestures  59602\n",
      "Number of non-touch gestures  6659\n"
     ]
    }
   ],
   "source": [
    "dirs = glob.glob(TRACES_PATH)\n",
    "touch_gesture_count = 0\n",
    "non_touch_gesture_count = 0\n",
    "for d in dirs:\n",
    "  with open(f'{d}/gestures.json') as f:\n",
    "    gestures = json.load(f)\n",
    "    gestures = [gestures[x] for x in sorted(gestures, key=lambda x: int(x))]\n",
    "    for gesture in gestures:\n",
    "        if is_touch_gesture(gesture):\n",
    "            touch_gesture_count += 1\n",
    "        else:\n",
    "            non_touch_gesture_count += 1\n",
    "print('Number of touch gestures ', touch_gesture_count)\n",
    "print('Number of non-touch gestures ', non_touch_gesture_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processes view hierarchies to construct dataset.\n",
    "# Extract texts from MAX_TOKEN elements from both view hierarchies.\n",
    "# Construct the dataset in the following format -\n",
    "# [[e11, e21, ... e1(MAX_TOKEN), e21, e22, ... e2(MAX_TOKEN), TARGET_TEXT], ...]\n",
    "def process_view_hierarchy(view_hierarchy1, view_hierarchy2, dataset, is_positive_sample = True):\n",
    "    if not view_hierarchy1 or not view_hierarchy2:\n",
    "        return dataset\n",
    "    \n",
    "    trace_path = view_hierarchy1.split('view_hierarchies')[0]\n",
    "    gesture_path = f'{trace_path}/gestures.json'\n",
    "    with open(gesture_path) as file:\n",
    "        gestures = json.load(file)\n",
    "\n",
    "    with open(view_hierarchy1) as file:\n",
    "        view_hierarchy1_json = json.load(file)\n",
    "    with open(view_hierarchy2) as file:\n",
    "        view_hierarchy2_json = json.load(file)\n",
    "    \n",
    "    if not view_hierarchy1_json or not view_hierarchy2_json:\n",
    "        return dataset\n",
    "\n",
    "    ui_number = view_hierarchy1.split('/')[-1].split('.')[0]\n",
    "    gesture = gestures[ui_number]\n",
    "    if not is_touch_gesture(gesture):\n",
    "        return dataset\n",
    "    \n",
    "    if not len(gesture):\n",
    "        return dataset\n",
    "    x_cord = gesture[0][0]\n",
    "    y_cord = gesture[0][1]\n",
    "    x = x_cord * DIM_X\n",
    "    y = y_cord * DIM_Y\n",
    "\n",
    "    leaf_nodes1 = get_all_leaf_nodes(view_hierarchy1_json)\n",
    "    leaf_nodes2 = get_all_leaf_nodes(view_hierarchy2_json)\n",
    "\n",
    "    target_text = get_target_text(leaf_nodes1, x, y)\n",
    "    if not target_text:\n",
    "        return dataset\n",
    "    \n",
    "    screen1_element = get_leaf_node_texts(leaf_nodes1)\n",
    "    screen2_element = get_leaf_node_texts(leaf_nodes2)\n",
    "\n",
    "    for i in range(len(screen1_element), MAX_TOKEN):\n",
    "        screen1_element.append(PLACEHOLDER_TEXT)\n",
    "    for i in range(len(screen2_element), MAX_TOKEN):\n",
    "        screen2_element.append(PLACEHOLDER_TEXT)\n",
    "\n",
    "    if is_positive_sample:\n",
    "        dataset.append(screen1_element + screen2_element)\n",
    "        y_true.append(1)\n",
    "    else:\n",
    "        dataset.append(screen1_element + screen2_element)\n",
    "        y_true.append(0)\n",
    "        \n",
    "    return dataset\n",
    "        \n",
    "\n",
    "def process_trace(trace_path, dataset):\n",
    "    view_hierarchies_path = f'{trace_path}/view_hierarchies/*'\n",
    "    view_hierarchies = sorted(glob.glob(view_hierarchies_path))\n",
    "    for i in range(len(view_hierarchies) - 1):\n",
    "        dataset = process_view_hierarchy(view_hierarchies[i], view_hierarchies[i+1], dataset)\n",
    "\n",
    "\n",
    "def add_negative_samples(dataset):\n",
    "    traces = sorted(glob.glob(TRACES_PATH))\n",
    "    total_positive_samples = len(dataset)\n",
    "    negative_samples_threshold = 1 * total_positive_samples\n",
    "    negative_samples_counter = 0\n",
    "    for i in range(len(traces) - 1):\n",
    "        trace_path1 = traces[i]\n",
    "        trace_path2 = traces[i+1]\n",
    "        view_hierarchies1_path = sorted(glob.glob(f'{trace_path1}/view_hierarchies/*'))\n",
    "        view_hierarchies2_path = sorted(glob.glob(f'{trace_path2}/view_hierarchies/*'))\n",
    "        for (view_hierarchy1, view_hierarchy2) in zip(view_hierarchies1_path, view_hierarchies2_path):\n",
    "            dataset = process_view_hierarchy(view_hierarchy1, view_hierarchy2, dataset, False)\n",
    "            negative_samples_counter += 1\n",
    "            if negative_samples_counter >= negative_samples_threshold:\n",
    "                return dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = []\n",
    "for trace_path in sorted(glob.glob(TRACES_PATH)):\n",
    "    process_trace(trace_path, dataset)\n",
    "\n",
    "dataset = add_negative_samples(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_words =  197635\n"
     ]
    }
   ],
   "source": [
    "# We create a custom standardization function to lowercase the text and \n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "# Define the number of words in a sequence.\n",
    "sequence_length = 1\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Set output_sequence_length length to pad all samples to same length.\n",
    "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "all_words = []\n",
    "for row in dataset:\n",
    "    for word in row[:-1]:\n",
    "        all_words.append(str(word))\n",
    "unique_words = set(all_words)\n",
    "print('unique_words = ',len(unique_words))\n",
    "vectorize_layer.adapt(list(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'and', 'of', 'you', 'a', 'your', 'in', 'or', 'for', 'is', 'with', 'on', 'this', 'by', 'that', 'be', 'any']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120690"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])\n",
    "len(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "text_vector_ds = text_ds.map(vectorize_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  118  8053   419  2296  8053   419  2296   394   394  2296 42026 34283\n",
      " 10910 71528  2296  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219   215  1320   428   428   215    37   215    37\n",
      "   215    37  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219] => ['settings', 'fancy', 'display', 'none', 'fancy', 'display', 'none', 'language', 'language', 'none', 'atrás', 'saltar', 'siguiente', 'settingsactivity', 'none', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'support', 'unable', 'remove', 'remove', 'support', 'if', 'support', 'if', 'support', 'if', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na']\n",
      "[ 1911  4443   393  1021 80366 24521 19797   604 86329 19797   536 11247\n",
      " 27125  7305  7305  2766   118   215    64  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  8005  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219] => ['pocket', 'constant', 'period', 'force', 'newtons', 'rigid', 'rotary', 'power', 'kinetic', 'rotary', 'center', 'harmonic', 'displacement', 'gravity', 'gravity', 'chapters', 'settings', 'support', 'about', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'linear', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na']\n",
      "[  538  2296    34   123  3798 90951  1397   869 82921  2811    32  2296\n",
      "   204     7   433   396   753   473   959   715  1281    31  1432  2144\n",
      "   991   682   741   562  1531  2139   940   574  1397  1453   878   695\n",
      "   698   153  2669  2669  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  3720   326   326    10   164  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219] => ['word', 'none', 'new', 'show', 'guess', 'hintलडाका', 't', 'answer', 'melanoma', 'chances', '1', 'none', 'next', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'z', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'hindi', 'login', 'login', 'or', 'try', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na']\n",
      "[ 5575   170   391  3145   797  3021 70605  4173  2053   245   122    58\n",
      "   118   250  2296  2296  3073  2296  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  5575   170   391   391   391  3145   797  3021\n",
      " 70605  4173  2053   245   122    58  2296  2296  3073  2296  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219  1219\n",
      "  1219  1219  1219  1219  1219  1219  1219  1219] => ['dev', 'home', 'book', 'checkin', 'flight', 'reservation', 'skypass', 'baggage', 'worldwide', 'contact', 'news', 'terms', 'settings', 'log', 'none', 'none', 'mileage', 'none', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'dev', 'home', 'book', 'book', 'book', 'checkin', 'flight', 'reservation', 'skypass', 'baggage', 'worldwide', 'contact', 'news', 'terms', 'none', 'none', 'mileage', 'none', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na']\n",
      "[  32   62  140   32   62 1219 1219 1219 1219 1219 1219 1219 1219 1219\n",
      " 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219\n",
      " 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219\n",
      " 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219\n",
      " 1219 1219 1219 1219 1219 1219 1219 1219  671 1219 1219 1219 1219 1219\n",
      " 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219\n",
      " 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219\n",
      " 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219\n",
      " 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219 1219\n",
      " 1219 1219] => ['1', '3', '6', '1', '3', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'currency', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na']\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "sequences = np.squeeze(sequences)\n",
    "\n",
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_dataset(sequences, inverse_vocab):\n",
    "    num_ns = len(inverse_vocab)\n",
    "    labels = []\n",
    "    input_data = []\n",
    "    for input_instance in sequences:\n",
    "        y = input_instance[-1:]\n",
    "        labels.append(y)\n",
    "        input_data.append(input_instance[:-1])\n",
    "#     categorized_labels = tf.keras.utils.to_categorical(labels, num_ns)\n",
    "    return tf.data.Dataset.from_tensor_slices((input_data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_reshaped = np.array(y_true).reshape(len(sequences), -1)\n",
    "sequences_stack = np.hstack((sequences, y_true_reshaped))\n",
    "train, test = train_test_split(sequences_stack, test_size=TEST_SIZE)\n",
    "train, val = train_test_split(sequences_stack, test_size=VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = map_to_dataset(train, inverse_vocab).batch(BATCH_SIZE)\n",
    "val = map_to_dataset(val, inverse_vocab).batch(BATCH_SIZE)\n",
    "test = map_to_dataset(test, inverse_vocab).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/271 [..............................] - ETA: 15s - loss: 0.5933 - accuracy: 0.7200WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0146s vs `on_train_batch_end` time: 0.1016s). Check your callbacks.\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 2/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 3/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 4/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 5/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 6/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 7/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 8/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 9/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 10/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 11/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 12/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 13/100\n",
      "271/271 [==============================] - 1s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 14/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 15/100\n",
      "271/271 [==============================] - 1s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 16/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 17/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 18/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 19/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 20/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 21/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 22/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 23/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 24/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 25/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 26/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 27/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 28/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 29/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 30/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 31/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 32/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 33/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 34/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 35/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 36/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 37/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 38/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 39/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 40/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 41/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 42/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 43/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 44/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 45/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 46/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 47/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 48/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 49/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 50/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 51/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 52/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 53/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 54/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 55/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 56/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 57/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 58/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 59/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 60/100\n",
      "271/271 [==============================] - 2s 7ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 61/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 62/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 63/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 64/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 65/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 66/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 67/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 68/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 69/100\n",
      "271/271 [==============================] - 2s 7ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 70/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 71/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 72/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 73/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 74/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 75/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 76/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 77/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 78/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 79/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 80/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 81/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 82/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 83/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 84/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 85/100\n",
      "271/271 [==============================] - 2s 6ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 86/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 87/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 88/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 89/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 90/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 91/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 92/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 93/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 94/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 95/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 96/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 97/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 98/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 99/100\n",
      "271/271 [==============================] - 1s 5ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n",
      "Epoch 100/100\n",
      "271/271 [==============================] - 1s 4ms/step - loss: 0.5878 - accuracy: 0.7255 - val_loss: 0.5781 - val_accuracy: 0.7352\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Flatten(input_shape=(MAX_TOKEN*2, )),\n",
    "    tf.keras.layers.Dense(100, name='Input'),\n",
    "    tf.keras.layers.Dense(100, activation='relu', name='Hidden'),\n",
    "    tf.keras.layers.Dense(1, activation='softmax', name='Softmax_Activation')\n",
    "])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train, validation_data=val, epochs=100, callbacks=[tensorboard_callback], batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "Hidden (Dense)               (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "Softmax_Activation (Dense)   (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 23,101\n",
      "Trainable params: 23,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
