{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary -\n",
    "# - There are totally 59K touch gestures.\n",
    "# - Only ~22K (37%) touch gestures was clicked on a leaf element which had text content.\n",
    "# - Created text dataset in the following format\n",
    "#   - [[e11, e21, ... e1(MAX_TOKEN), e21, e22, ... e2(MAX_TOKEN), TARGET_TEXT],\n",
    "#      ...\n",
    "#     ]\n",
    "#   - Vectorized the dataset.\n",
    "# - Tried a simple classification model with a single hidden layer(1024).\n",
    "#   - Accuracy on train data : 48%\n",
    "#   - Accuracy on validation (20%) : 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements.\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants.\n",
    "LONG_TOUCH_THRESHOLD = 5\n",
    "DIM_X = 1440\n",
    "DIM_Y = 2560\n",
    "MAX_TOKEN = 64\n",
    "BATCH_SIZE = 10000\n",
    "BUFFER_SIZE = 100\n",
    "VOCAB_SIZE = 500\n",
    "TRAIN_SIZE = 0.8\n",
    "VAL_SIZE = 0.1\n",
    "TEST_SIZE = 0.1\n",
    "TRACES_PATH = 'filtered_traces/*/*'\n",
    "NEGATIVE_SAMPLE_TARGET = '[null]'\n",
    "PLACEHOLDER_TEXT = 'n/a'\n",
    "y_true = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets all leaf nodes for a given element.\n",
    "def get_leaf_nodes(element, leaf_nodes):\n",
    "    if not element:\n",
    "        return leaf_nodes\n",
    "    if 'children' not in element:\n",
    "        leaf_nodes.append(element)\n",
    "        return leaf_nodes\n",
    "    for child in element['children']:\n",
    "        get_leaf_nodes(child, leaf_nodes)\n",
    "    return leaf_nodes\n",
    "\n",
    "\n",
    "def get_all_leaf_nodes(view_hierarchy_json):\n",
    "    activity = view_hierarchy_json.get('activity')\n",
    "    if not activity:\n",
    "        return dataset\n",
    "    root = activity.get('root')\n",
    "    return get_leaf_nodes(root, [])\n",
    "\n",
    "\n",
    "def get_target_text(leaf_nodes, x, y):\n",
    "    target_text = None\n",
    "    for leaf_node in leaf_nodes:\n",
    "        bounds = leaf_node['bounds']\n",
    "        if bounds[0] <= x and bounds[2] >= x and bounds[1] <= y and bounds[3] >= y:\n",
    "            if 'text' in leaf_node:\n",
    "                target_text = leaf_node['text'] or leaf_node.get('text-hint')\n",
    "    return target_text\n",
    "\n",
    "\n",
    "def get_leaf_node_texts(leaf_nodes):\n",
    "    i = 1\n",
    "    element_texts = []\n",
    "    for leaf_node in leaf_nodes:\n",
    "        if 'text' in leaf_node:\n",
    "            text = leaf_node['text'] or leaf_node.get('text-hint')\n",
    "            element_texts.append(str(text))\n",
    "            i += 1\n",
    "            if i == MAX_TOKEN:\n",
    "                break\n",
    "    return element_texts\n",
    "\n",
    "\n",
    "# Identifies if a given gesture is a TOUCH gesture.\n",
    "# In this task, we will only be focussing on TOUCH gestures.\n",
    "def is_touch_gesture(gesture):\n",
    "    if len(gesture) <= LONG_TOUCH_THRESHOLD:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of touch gestures  59602\n",
      "Number of non-touch gestures  6659\n"
     ]
    }
   ],
   "source": [
    "dirs = glob.glob(TRACES_PATH)\n",
    "touch_gesture_count = 0\n",
    "non_touch_gesture_count = 0\n",
    "for d in dirs:\n",
    "  with open(f'{d}/gestures.json') as f:\n",
    "    gestures = json.load(f)\n",
    "    gestures = [gestures[x] for x in sorted(gestures, key=lambda x: int(x))]\n",
    "    for gesture in gestures:\n",
    "        if is_touch_gesture(gesture):\n",
    "            touch_gesture_count += 1\n",
    "        else:\n",
    "            non_touch_gesture_count += 1\n",
    "print('Number of touch gestures ', touch_gesture_count)\n",
    "print('Number of non-touch gestures ', non_touch_gesture_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processes view hierarchies to construct dataset.\n",
    "# Extract texts from MAX_TOKEN elements from both view hierarchies.\n",
    "# Construct the dataset in the following format -\n",
    "# [[e11, e21, ... e1(MAX_TOKEN), e21, e22, ... e2(MAX_TOKEN), TARGET_TEXT], ...]\n",
    "def process_view_hierarchy(view_hierarchy1, view_hierarchy2, dataset, is_positive_sample = True):\n",
    "    if not view_hierarchy1 or not view_hierarchy2:\n",
    "        return dataset\n",
    "    \n",
    "    trace_path = view_hierarchy1.split('view_hierarchies')[0]\n",
    "    gesture_path = f'{trace_path}/gestures.json'\n",
    "    with open(gesture_path) as file:\n",
    "        gestures = json.load(file)\n",
    "\n",
    "    with open(view_hierarchy1) as file:\n",
    "        view_hierarchy1_json = json.load(file)\n",
    "    with open(view_hierarchy2) as file:\n",
    "        view_hierarchy2_json = json.load(file)\n",
    "    \n",
    "    if not view_hierarchy1_json or not view_hierarchy2_json:\n",
    "        return dataset\n",
    "\n",
    "    ui_number = view_hierarchy1.split('/')[-1].split('.')[0]\n",
    "    gesture = gestures[ui_number]\n",
    "    if not is_touch_gesture(gesture):\n",
    "        return dataset\n",
    "    \n",
    "    if not len(gesture):\n",
    "        return dataset\n",
    "    x_cord = gesture[0][0]\n",
    "    y_cord = gesture[0][1]\n",
    "    x = x_cord * DIM_X\n",
    "    y = y_cord * DIM_Y\n",
    "\n",
    "    leaf_nodes1 = get_all_leaf_nodes(view_hierarchy1_json)\n",
    "    leaf_nodes2 = get_all_leaf_nodes(view_hierarchy2_json)\n",
    "\n",
    "    target_text = get_target_text(leaf_nodes1, x, y)\n",
    "    if not target_text:\n",
    "        return dataset\n",
    "    \n",
    "    screen1_element = get_leaf_node_texts(leaf_nodes1)\n",
    "    screen2_element = get_leaf_node_texts(leaf_nodes2)\n",
    "\n",
    "    for i in range(len(screen1_element), MAX_TOKEN):\n",
    "        screen1_element.append(PLACEHOLDER_TEXT)\n",
    "    for i in range(len(screen2_element), MAX_TOKEN):\n",
    "        screen2_element.append(PLACEHOLDER_TEXT)\n",
    "\n",
    "    if is_positive_sample:\n",
    "        dataset.append(screen1_element + screen2_element)\n",
    "        y_true.append(1)\n",
    "    else:\n",
    "        dataset.append(screen1_element + screen2_element)\n",
    "        y_true.append(0)\n",
    "        \n",
    "    return dataset\n",
    "        \n",
    "\n",
    "def process_trace(trace_path, dataset):\n",
    "    view_hierarchies_path = f'{trace_path}/view_hierarchies/*'\n",
    "    view_hierarchies = sorted(glob.glob(view_hierarchies_path))\n",
    "    for i in range(len(view_hierarchies) - 1):\n",
    "        dataset = process_view_hierarchy(view_hierarchies[i], view_hierarchies[i+1], dataset)\n",
    "\n",
    "\n",
    "def add_negative_samples(dataset):\n",
    "    traces = sorted(glob.glob(TRACES_PATH))\n",
    "    total_positive_samples = len(dataset)\n",
    "    negative_samples_threshold = 1 * total_positive_samples\n",
    "    negative_samples_counter = 0\n",
    "    for i in range(len(traces) - 1):\n",
    "        trace_path1 = traces[i]\n",
    "        trace_path2 = traces[-i]\n",
    "        view_hierarchies1_path = sorted(glob.glob(f'{trace_path1}/view_hierarchies/*'))\n",
    "        view_hierarchies2_path = sorted(glob.glob(f'{trace_path2}/view_hierarchies/*'))\n",
    "        for (view_hierarchy1, view_hierarchy2) in zip(view_hierarchies1_path, view_hierarchies2_path):\n",
    "            dataset = process_view_hierarchy(view_hierarchy1, view_hierarchy2, dataset, False)\n",
    "            negative_samples_counter += 1\n",
    "            if negative_samples_counter >= negative_samples_threshold:\n",
    "                return dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = []\n",
    "for trace_path in sorted(glob.glob(TRACES_PATH)):\n",
    "    process_trace(trace_path, dataset)\n",
    "\n",
    "dataset = add_negative_samples(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_words =  198807\n"
     ]
    }
   ],
   "source": [
    "# We create a custom standardization function to lowercase the text and \n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "# Define the number of words in a sequence.\n",
    "sequence_length = 1\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Set output_sequence_length length to pad all samples to same length.\n",
    "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "all_words = []\n",
    "for row in dataset:\n",
    "    for word in row[:-1]:\n",
    "        all_words.append(str(word))\n",
    "unique_words = set(all_words)\n",
    "print('unique_words = ',len(unique_words))\n",
    "vectorize_layer.adapt(list(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'and', 'of', 'you', 'a', 'in', 'your', 'or', 'for', 'is', 'with', 'on', 'this', 'by', 'that', 'any', 'be']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123018"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])\n",
    "len(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "text_vector_ds = text_ds.map(vectorize_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  125  8628   384  2367  8628   384  2367   383   383  2367 28400 34968\n",
      " 13233 73848  2367  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370   207  1361   434   434   207    34   207    34\n",
      "   207    34  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370] => ['settings', 'fancy', 'display', 'none', 'fancy', 'display', 'none', 'language', 'language', 'none', 'atrás', 'saltar', 'siguiente', 'settingsactivity', 'none', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'support', 'unable', 'remove', 'remove', 'support', 'if', 'support', 'if', 'support', 'if', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na']\n",
      "[ 2117  4655   431  1090 82641 24884 20055   579 88559 20055   570 11303\n",
      " 27475  6500  6500  2934   125   207    66  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  7360  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370] => ['pocket', 'constant', 'period', 'force', 'newtons', 'rigid', 'rotary', 'power', 'kinetic', 'rotary', 'center', 'harmonic', 'displacement', 'gravity', 'gravity', 'chapters', 'settings', 'support', 'about', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'linear', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na']\n",
      "[  591  2367    35   128  4068 93232  1374   904 85185  2846    31  2367\n",
      "   217     7   378   334   681   487  1009   804  1400    32  1490  2216\n",
      "  1066   709   766   663  1545  2254   971   569  1374  1612  1034   723\n",
      "   735   175  2731  2731  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  4218   339   339    10   172  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370] => ['word', 'none', 'new', 'show', 'guess', 'hintलडाका', 't', 'answer', 'melanoma', 'chances', '1', 'none', 'next', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'z', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'hindi', 'login', 'login', 'or', 'try', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na']\n",
      "[ 5666   173   410  2983   785  3152 72919  4255  1229   251   130    48\n",
      "   125   274  2367  2367  3107  2367  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  5666   173   410   410   410  2983   785  3152\n",
      " 72919  4255  1229   251   130    48  2367  2367  3107  2367  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370  1370\n",
      "  1370  1370  1370  1370  1370  1370  1370  1370] => ['dev', 'home', 'book', 'checkin', 'flight', 'reservation', 'skypass', 'baggage', 'worldwide', 'contact', 'news', 'terms', 'settings', 'log', 'none', 'none', 'mileage', 'none', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'dev', 'home', 'book', 'book', 'book', 'checkin', 'flight', 'reservation', 'skypass', 'baggage', 'worldwide', 'contact', 'news', 'terms', 'none', 'none', 'mileage', 'none', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na']\n",
      "[  31   60  134   31   60 1370 1370 1370 1370 1370 1370 1370 1370 1370\n",
      " 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370\n",
      " 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370\n",
      " 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370\n",
      " 1370 1370 1370 1370 1370 1370 1370 1370  697 1370 1370 1370 1370 1370\n",
      " 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370\n",
      " 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370\n",
      " 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370\n",
      " 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370 1370\n",
      " 1370 1370] => ['1', '3', '6', '1', '3', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'currency', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na', 'na']\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "sequences = np.squeeze(sequences)\n",
    "\n",
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_dataset(sequences, inverse_vocab):\n",
    "    num_ns = len(inverse_vocab)\n",
    "    labels = []\n",
    "    input_data = []\n",
    "    for input_instance in sequences:\n",
    "        y = input_instance[-1:]\n",
    "        labels.append(y)\n",
    "        input_data.append(input_instance[:-1])\n",
    "#     categorized_labels = tf.keras.utils.to_categorical(labels, num_ns)\n",
    "    return tf.data.Dataset.from_tensor_slices((input_data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_reshaped = np.array(y_true).reshape(len(sequences), -1)\n",
    "sequences_stack = np.hstack((sequences, y_true_reshaped))\n",
    "train, test = train_test_split(sequences_stack, test_size=TEST_SIZE)\n",
    "train, val = train_test_split(sequences_stack, test_size=VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "TAKE_SIZE = 100000\n",
    "train = map_to_dataset(train, inverse_vocab).take(TAKE_SIZE).batch(BATCH_SIZE)\n",
    "val = map_to_dataset(val, inverse_vocab).take(TAKE_SIZE).batch(BATCH_SIZE)\n",
    "test = map_to_dataset(test, inverse_vocab).take(TAKE_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 0.5854 - binary_accuracy: 0.7279WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1122s vs `on_train_batch_end` time: 0.4117s). Check your callbacks.\n",
      "3/3 [==============================] - 1s 300ms/step - loss: 0.5848 - binary_accuracy: 0.7284 - val_loss: 0.5774 - val_binary_accuracy: 0.7359\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5848 - binary_accuracy: 0.7284 - val_loss: 0.5774 - val_binary_accuracy: 0.7359\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5848 - binary_accuracy: 0.7284 - val_loss: 0.5774 - val_binary_accuracy: 0.7359\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.5848 - binary_accuracy: 0.7284 - val_loss: 0.5774 - val_binary_accuracy: 0.7359\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5848 - binary_accuracy: 0.7284 - val_loss: 0.5774 - val_binary_accuracy: 0.7359\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5848 - binary_accuracy: 0.7284 - val_loss: 0.5774 - val_binary_accuracy: 0.7359\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.5848 - binary_accuracy: 0.7284 - val_loss: 0.5774 - val_binary_accuracy: 0.7359\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5848 - binary_accuracy: 0.7284 - val_loss: 0.5774 - val_binary_accuracy: 0.7359\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.5848 - binary_accuracy: 0.7284 - val_loss: 0.5774 - val_binary_accuracy: 0.7359\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5848 - binary_accuracy: 0.7284 - val_loss: 0.5774 - val_binary_accuracy: 0.7359\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Flatten(input_shape=(MAX_TOKEN*2, )),\n",
    "    tf.keras.layers.Dense(128, name='Input', activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu', name='Hidden'),\n",
    "    tf.keras.layers.Dropout(.4),\n",
    "    tf.keras.layers.Dense(1, activation='softmax', name='Softmax_Activation')\n",
    "])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "history = model.fit(train, validation_data=val, epochs=10, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "Hidden (Dense)               (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Softmax_Activation (Dense)   (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 33,153\n",
      "Trainable params: 33,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
